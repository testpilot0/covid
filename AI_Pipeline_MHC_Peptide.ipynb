{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHC Peptide Binding Pipeline\n",
    "### Supporting research: Covid19 and beyond for vaccine candidates\n",
    "Cloud AI Platform Pipelines provides a way to deploy robust, repeatable machine learning pipelines along with monitoring, auditing, version tracking, and reproducibility, and delivers an enterprise-ready, easy to install, secure execution environment for your ML workflows. \n",
    "The goal is to democratize machine learning by enabling SQL practitioners to build models using their existing tools and to increase development speed by eliminating the need for data movement. In this tutorial, you use the sample Covid19 dataset for BigQuery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Set up your environment.\n",
    "\n",
    "AI Platform Pipelines will prepare a development environment to build a pipeline, and a Kubeflow Pipeline cluster to run the newly built pipeline.\n",
    "\n",
    "**NOTE:** To select a particular TensorFlow version, or select a GPU instance, create a TensorFlow pre-installed instance in AI Platform Notebooks.\n",
    "\n",
    "**NOTE:** There might be some errors during package installation. For example: \n",
    "\n",
    ">\"ERROR: some-package 0.some_version.1 has requirement other-package!=2.0.,&lt;3,&gt;=1.15, but you'll have other-package 2.0.0 which is incompatible.\" Please ignore these errors at this moment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `tfx`, `kfp`, and `skaffold`, and add installation path to the `PATH` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNiqq_kN0okj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: google-cloud-storage 1.26.0 has requirement google-resumable-media<0.6dev,>=0.5.0, but you'll have google-resumable-media 0.4.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-cloud-bigquery 1.17.1 has requirement google-resumable-media<0.5.0dev,>=0.3.1, but you'll have google-resumable-media 0.5.0 which is incompatible.\u001b[0m\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 39.2M  100 39.2M    0     0  58.6M      0 --:--:-- --:--:-- --:--:-- 58.6M\n"
     ]
    }
   ],
   "source": [
    "# Install tfx and kfp Python packages.\n",
    "!pip install --user --upgrade -q tfx==0.21.2\n",
    "!pip install --user --upgrade -q kfp==0.2.5\n",
    "# Download skaffold and set it executable.\n",
    "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && mv skaffold /home/jupyter/.local/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43ncix2Q0okm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include user python binary directory and a directory containing `skaffold`.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAIoKMNG0okq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.21.2\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Check GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw3nsooU0okv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCP project ID:covid-19-271622\n"
     ]
    }
   ],
   "source": [
    "# Read GCP project id from env.\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GCP_PROJECT_ID=shell_output[0]\n",
    "print(\"GCP project ID:\" + GCP_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to access your KFP cluster. You can access it in your Google Cloud Console under \"AI Platform > Pipeline\" menu. The \"endpoint\" of the KFP cluster can be found from the URL of the Pipelines dashboard, or you can get it from the URL of the Getting Started page where you launched this notebook. Let's create an `ENDPOINT` environment variable and set it to the KFP cluster endpoint. **ENDPOINT should contain only the hostname part of the URL.** For example, if the URL of the KFP dashboard is `https://1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com/#/start`, ENDPOINT value becomes `1e9deb537390ca22-dot-asia-east1.pipelines.googleusercontent.com`.\n",
    "\n",
    ">**NOTE: You MUST set your ENDPOINT value below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Set AI Pipeline Kube cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzqEQORV0oky"
   },
   "outputs": [],
   "source": [
    "# This refers to the KFP cluster endpoint\n",
    "ENDPOINT='4dfa62a617d46f32-dot-us-central2.pipelines.googleusercontent.com' # Enter your ENDPOINT here.\n",
    "if not ENDPOINT:\n",
    "    from absl import logging\n",
    "    logging.error('Set your ENDPOINT in this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 set custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ztxXOVD0ok4"
   },
   "outputs": [],
   "source": [
    "# Docker image name for the pipeline image \n",
    "CUSTOM_TFX_IMAGE='gcr.io/' + GCP_PROJECT_ID + '/tfx-pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Copy the predefined template to your project directory.\n",
    "\n",
    "In this step, we will create a working pipeline project directory and files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a **different name by changing the `PIPELINE_NAME` below.** This will also become the name of the project directory where your files will be put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Set pipeline name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIPlt-700ok-"
   },
   "outputs": [],
   "source": [
    "PIPELINE_NAME=\"mch_pipeline\"\n",
    "import os\n",
    "PROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"AIHub\",PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TFX pipeline includes the `mhc peptide` template with the TFX python package. If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "We will be copying pre-built Peptide Prediction pipeline files from shared GCS bucket. You can also use\n",
    "the `tfx template copy` CLI command copies predefined template files into your project directory.\n",
    "\n",
    "**Note:** Create a folder called '**mhc_pipeline**' under AIHub on the left panel file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 run once to copy template files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLXpTTjU0olD"
   },
   "outputs": [],
   "source": [
    "#!tfx template copy \\\n",
    "#  --pipeline_name={PIPELINE_NAME} \\\n",
    "#  --destination_path={PROJECT_DIR} \\\n",
    "#  --model=taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Validate setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P-HljcU0olI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/AIHub/mch_pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Check files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YA9MFs7f0olR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_dag_runner.py     features.py\t       model.py\n",
      "build.yaml\t       features_test.py        model_test.py\n",
      "configs.py\t       hparams.py\t       old\n",
      "data\t\t       __init__.py\t       pipeline.py\n",
      "data_validation.ipynb  kubeflow_dag_runner.py  preprocessing.py\n",
      "deployedmhc\t       mch_pipeline.tar.gz     preprocessing_test.py\n",
      "Dockerfile\t       model_analysis.ipynb    __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Test template feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0cMdE2Z0olU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.7.6: /opt/conda/bin/python3\n",
      "[ RUN      ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "[  FAILED  ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "[ RUN      ] FeaturesTest.testTransformedNames\n",
      "[       OK ] FeaturesTest.testTransformedNames\n",
      "[ RUN      ] FeaturesTest.test_session\n",
      "[  SKIPPED ] FeaturesTest.test_session\n",
      "======================================================================\n",
      "FAIL: testNumberOfBucketFeatureBucketCount (__main__.FeaturesTest)\n",
      "testNumberOfBucketFeatureBucketCount (__main__.FeaturesTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"features_test.py\", line 33, in testNumberOfBucketFeatureBucketCount\n",
      "    len(features.CATEGORICAL_FEATURE_MAX_VALUES))\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.008s\n",
      "\n",
      "FAILED (failures=1, skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python3 features_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Review GCS bucket list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgxK0s220olZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts.covid-19-271622.appspot.com\n",
      "cancer_vaccine\n",
      "dataproc-staging-us-central1-598002519658-gxbdggqa\n",
      "dataproc-temp-us-central1-598002519658-hu8gmvbe\n",
      "hla_peptide_dataset\n",
      "hostedkfp-default-y95ed9e0de\n",
      "hostedkfp-default-yfu8n9ppw9\n",
      "mhc_peptide_model\n",
      "mhc_pipelines\n",
      "virus_mutations\n"
     ]
    }
   ],
   "source": [
    "# You can see your buckets using `gsutil`. Following command will show bucket names without prefix and postfix.\n",
    "!gsutil ls | cut -d / -f 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create pipeline based on template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOU7zQof0olf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Target image gcr.io/covid-19-271622/tfx-pipeline is not used. If the build spec is provided, update the target image in the build spec file build.yaml.\n",
      "Use skaffold to build the container image.\n",
      "/home/jupyter/.local/bin/skaffold\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "INFO:absl:Neither eval_config nor feature_slicing_spec is passed, the model is treated as estimator.\n",
      "WARNING:absl:feature_slicing_spec is deprecated, please use eval_config instead.\n",
      "WARNING:tensorflow:From /home/jupyter/AIHub/mch_pipeline/pipeline.py:139: ModelValidator.__init__ (from tfx.components.model_validator.component) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "ModelValidator is deprecated, use Evaluator instead.\n",
      "INFO:absl:Adding upstream dependencies for component BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ModelValidator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: ModelValidator\n",
      "Pipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/AIHub/mch_pipeline/mch_pipeline.tar.gz\n",
      "Pipeline \"mch_pipeline\" already exists.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline_path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build_target_image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Run an experiment on a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKSjVVsa0oli"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: mch_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: mch_pipeline\n",
      "+-----------------+--------------------------------------+----------+---------------------------+\n",
      "| pipeline_name   | run_id                               | status   | created_at                |\n",
      "+=================+======================================+==========+===========================+\n",
      "| mch_pipeline    | bdb9cce8-53a8-4685-802e-efdea4f26f9d |          | 2020-04-29T18:22:06+00:00 |\n",
      "+-----------------+--------------------------------------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline_name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Update pipeline and run an experiment for modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE-Pqvto0olm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Use skaffold to build the container image.\n",
      "/home/jupyter/.local/bin/skaffold\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "INFO:absl:Neither eval_config nor feature_slicing_spec is passed, the model is treated as estimator.\n",
      "WARNING:absl:feature_slicing_spec is deprecated, please use eval_config instead.\n",
      "WARNING:tensorflow:From /home/jupyter/AIHub/mch_pipeline/pipeline.py:139: ModelValidator.__init__ (from tfx.components.model_validator.component) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "ModelValidator is deprecated, use Evaluator instead.\n",
      "INFO:absl:Adding upstream dependencies for component BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component ModelValidator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: ModelValidator\n",
      "Pipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/AIHub/mch_pipeline/mch_pipeline.tar.gz\n",
      "{'created_at': datetime.datetime(2020, 5, 12, 20, 38, 40, tzinfo=tzlocal()),\n",
      " 'default_version': {'code_source_url': None,\n",
      "                     'created_at': datetime.datetime(2020, 5, 12, 20, 38, 40, tzinfo=tzlocal()),\n",
      "                     'id': '73a6f1b3-761c-4efb-8a80-62e79493e994',\n",
      "                     'name': 'mch_pipeline',\n",
      "                     'package_url': None,\n",
      "                     'parameters': [{'name': 'pipeline-root',\n",
      "                                     'value': 'gs://mhc_pipelines/tfx_pipeline_output/mch_pipeline'}],\n",
      "                     'resource_references': [{'key': {'id': '73a6f1b3-761c-4efb-8a80-62e79493e994',\n",
      "                                                      'type': 'PIPELINE'},\n",
      "                                              'name': None,\n",
      "                                              'relationship': 'OWNER'}]},\n",
      " 'description': None,\n",
      " 'error': None,\n",
      " 'id': '73a6f1b3-761c-4efb-8a80-62e79493e994',\n",
      " 'name': 'mch_pipeline',\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://mhc_pipelines/tfx_pipeline_output/mch_pipeline'}],\n",
      " 'url': None}\n",
      "Pipeline \"mch_pipeline\" updated successfully.\n",
      "CLI\n",
      "Creating a run for pipeline: mch_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: mch_pipeline\n",
      "+-----------------+--------------------------------------+----------+---------------------------+\n",
      "| pipeline_name   | run_id                               | status   | created_at                |\n",
      "+=================+======================================+==========+===========================+\n",
      "| mch_pipeline    | 6d573a2e-9fd7-4f10-8109-230f18011ca1 |          | 2020-05-12T20:38:47+00:00 |\n",
      "+-----------------+--------------------------------------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "!tfx pipeline update \\\n",
    "--pipeline_path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "# You can run the pipeline the same way.\n",
    "!tfx run create --pipeline_name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Update only  command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE-Pqvto0olm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: tfx: not found\n"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "!tfx pipeline update \\\n",
    "--pipeline_path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Run updated pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE-Pqvto0olm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: mch_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: mch_pipeline\n",
      "+-----------------+--------------------------------------+----------+---------------------------+\n",
      "| pipeline_name   | run_id                               | status   | created_at                |\n",
      "+=================+======================================+==========+===========================+\n",
      "| mch_pipeline    | 86b08b26-b24f-48fd-b77f-ff37206c3535 |          | 2020-03-28T02:02:25+00:00 |\n",
      "+-----------------+--------------------------------------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "# You can run the pipeline the same way.\n",
    "!tfx run create --pipeline_name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
